<!-- 
In Consistent Hashing we hashes both request and server with same hashing function in same range of values.
    [ server_index where server will go = hash(serverId)  % M ]
    [ key_index   where request will go = hash(requestId) % M ]

    M = we use a circular hash space (0 to 2Â³Â² - 1)
    Hash function = SHA-1, MurmurHash.
-->

<!--
    let M= 30 (Size of Ring)

    Step 1: Map Severs on Ring using Same Hash Function :
    ðŸŸ  server 0  s0  | Id- 0  =>  Server_Index = hash(s1) % 30 =  hash(0) % 30  = 49 % 30  =>  23
    ðŸŸ  server 1  s1  | Id- 1  =>  Server_Index = hash(s2) % 30 =  hash(1) % 30  = 67 % 30  =>  10
    ðŸŸ  server 2  s2  | Id- 2  =>  Server_Index = hash(s3) % 30 =  hash(2) % 30  = 72 % 30  =>  25
    ðŸŸ  server 3  s3  | Id- 3  =>  Server_Index = hash(s3) % 30 =  hash(3) % 30  = 12 % 30  =>  29

    Step 2: Map Requests on Ring using Same Hash Function:
    ðŸ”µ Request 0  r0  | Id- 0  =>  Request_Index = hash(r0) % 30 =  hash(0) % 30  = 0 % 30  =>  0
    ðŸ”µ Request 1  r1  | Id- 1  =>  Request_Index = hash(r1) % 30 =  hash(1) % 30  = 1 % 30  =>  1 
    ðŸ”µ Request 2  r2  | Id- 2  =>  Request_Index = hash(r2) % 30 =  hash(2) % 30  = 2 % 30  =>  2 
    ðŸ”µ Request 3  r3  | Id-26  =>  Request_Index = hash(r2) % 30 =  hash(26) % 30 = 26 % 30 =>  26

        r0       r1
         ðŸ”µâ”€â”€â”€â”€â”€â”€ðŸ”µâ”€â”€â”€ðŸ”µ r2          ---
         /                \               \
  (29)s4ðŸ”´                  \              \
      /                   ðŸ”´ s2 (10)        |
      |                     |                | -> Clockwise Direction  Mapping
 r3 ðŸ”µ                     ðŸ”µ r4
      \                     |
       \                   /
         \               /
      s3 ðŸ”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ðŸ”´ s1 (23)   || Consistent Hashing Ring
     (25)

    Step 3: Every Request will take its nearest server in clockwise direction for load balancing.
            r0, r1, r2 -> s2 ðŸ”´
            r4         -> s1 ðŸ”´
            r3         -> s4 ðŸ”´

     There is no load on s3 ðŸ”´


Advantage Of Consistent Hashing :
--------------------------------
1. With simple hashing, when a new server is added, almost all the requests need to be remapped.
   With consistent hashing, adding a new server only requires redistribution of a fraction of the requests.

   Suppose a new server s5 is added.
   The new server will take responsibility for some keys from its successor server, reducing load imbalance.

        r0     s5   r1
         ðŸ”µâ”€â”€â”€ðŸ”´â”€â”€ðŸ”µâ”€â”€â”€ðŸ”µ r2         
         /                \               
  (29)s4ðŸ”´                  \              
      /                   ðŸ”´ s2 (10)        
      |                     |               
 r3 ðŸ”µ                     ðŸ”µ r4
      \                     |
       \                   /
         \               /
      s3 ðŸ”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ðŸ”´ s1 (23)   
     (25)

            r0         -> s5 ðŸ”´  ( New Request alignment)

            --  r1, r2 -> s2 ðŸ”´  ( Unaffected : Request alignment)
            r4         -> s1 ðŸ”´  ( Unaffected : Request alignment)
            r3         -> s4 ðŸ”´  ( Unaffected : Request alignment)
      There is no load on s3 ðŸ”´  ( Unaffected : Request alignment)

           
            
      

2. If a node fails, its keys are reassigned to the next available node, reducing rehashing to only a fraction of the keys.

        r0     s5   r1
         ðŸ”µâ”€â”€â”€ðŸ”´â”€â”€ðŸ”µâ”€â”€â”€ðŸ”µ r2         
         /                \               
  (29)s4ðŸ”´                  \              
      /                     âŒ ------------------- > ðŸ”´ s2 (10)  Failed Server  
      |                     |               
 r3 ðŸ”µ                     ðŸ”µ r4
      \                     |
       \                   /
         \               /
      s3 ðŸ”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ðŸ”´ s1 (23)   
     (25)

            r1, r2     -> s1 ðŸ”´  ( New Request alignment)
            r4         -> s1 ðŸ”´  ( Unaffected : Request alignment)

            r3         -> s4 ðŸ”´  ( Unaffected : Request alignment)
            r0         -> s5 ðŸ”´  ( Unaffected : Request alignment)
      There is no load on s3 ðŸ”´  ( Unaffected : Request alignment)


      Huge Problem In this :
      -----------------------
      Load on Server s1 = r1, r2, r4
      here half of load of sytem is on Server S1


Solution :
----------
  In consistent hashing with virtual nodes, each physical server is mapped to multiple virtual nodes to improve load balancing. 
  This ensures that when a server is added or removed, the distribution of request remains more unaffected.


    ðŸŸª server 0 (s0) â†’ Virtual Nodes: v0-1, v0-2, v0-3
    ðŸ”µ server 1 (s1) â†’ Virtual Nodes: v1-1, v1-2, v1-3
    ðŸŸ£ server 2 (s2) â†’ Virtual Nodes: v2-1, v2-2, v2-3
    ðŸŸ  server 3 (s3) â†’ Virtual Nodes: v3-1, v3-2, v3-3

    Consistent Hashing Ring with Virtual Nodes

                    ðŸ”µ r0
            v0-1 ðŸŸªâ”€â”€â”€â”€â”€â”€â”€ðŸ”´ v1-1
            /                  \
            /                    \
    v3-2 ðŸŸ                       ðŸ”´ v1-2
        |                          |
   ðŸ”µr0|                           | ðŸ”µ r2
        |                          |
    v2-1 ðŸŸ£                      ðŸ”´ v1-3
            \                      /
            \                    /
            v2-2 ðŸŸ£â”€â”€â”€â”€â”€â”€â”€ðŸŸ  v3-1
                    ðŸ”µr3

    Explanation:
    - Each server is represented by multiple virtual nodes.
    - This helps distribute keys more evenly.
    - If a server fails or is removed, the impact is minimized as its virtual nodes are spread across the ring.

    Key mappings:
    r0 â†’ v3-2
    r0 â†’ v1-1
    r2 â†’ v1-3
    r3 â†’ v2-2


Note: When a physical server (real machine) goes down, all of its virtual nodes (vNodes) also disappear from the ring. 
----  However, since virtual nodes are evenly distributed across the ring, the impact is still smaller compared to traditional consistent hashing.

      This means:
      Instead of losing all keys mapped to a single physical server, the keys get redistributed to the remaining virtual nodes of other servers.
      The system remains more balanced because other virtual nodes from different servers are still present in the ring.
-->
